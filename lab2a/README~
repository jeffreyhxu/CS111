NAME: Jeffrey Xu
EMAIL: jeffreyhxu@gmail.com
ID: 404768745

This tarball contains lab2_add.c, SortedList.h, SortedList.c, lab2_list.c,
Makefile, lab2_add.csv, lab2_list.csv, lab2_add-1.png, lab2_add-2.png,
lab2_add-3.png, lab2_add-4.png, lab2_add-5.png, lab2_list-1.png,
lab2_list-2.png, lab2_list-3.png, lab2_list-4.png, lab2_add.gp, lab2_list.gp,
and README.

lab2_add.c is the source code for a program that tests the behavior of a
function that adds to a variable shared across multiple threads. It supports
--threads=# and --iterations=# options to choose the number of threads and
repetitions of the add function, a --yield option to make each thread yield in
between loading the value of the variable and storing the new value back into
the variable, and a --sync=(m|s|c) option to choose between using a mutex, a
spinlock, and gcc's builtin atomic compare-and-swap to prevent conflicts between
the threads.

SortedList.h

SortedList.c

lab2_list.c

Makefile supports the default argument, which compiles all programs, tests,
which runs test cases on each program to generate csv files, graphs, which uses
provided gnuplot scripts (slightly modified to fix the path for gnuplot) to
generate graphs, dist, which creates the deliverable tarball, and clean, which
deletes all programs and output created by the Makefile.

lab2_add.csv contains the results created by the tests of lab2_add.

lab2_list.csv contains the results created by the tests of lab2_list.

lab2_add-1.png is a plot of the successful runs by number of threads and
iterations, with and without yields, showing by omission which runs failed.

lab2_add-2.png is a plot of the average time per operation with and without
yields.

lab2_add-3.png is a plot of the average time per operation using one thread
against the total number of operations.

lab2_add-4.png is a plot of the successful runs by number of threads and
iterations with yields for each synchronization option, with no synchronization
shown for contrast. Note that there are much fewer tests for the synchronized
runs, as it is only necessary to show that they succeed even at high iterations.

lab2_add-5.png is a plot of the average time per operation against the number of
threads for each synchronization option.

lab2_list-1.png is a plot of the average time per unprotected operation on one
thread against the number of iterations, corrected and uncorrected for list
length.

lab2_list-2.png is a plot of the successful runs by threads and iterations with
and without yields.

lab2_list-3.png is a plot of the successful runs with synchronization options
with fixed thread and iteration count against the yield options.

lab2_list-4.png is a plot of the average length-corrected time per operation
against the number of threads for each synchronization option.

lab2_add.gp is the provided data reduction script for the tests of lab2_add,
modified to fix the path to gnuplot.

lab2_list.gp is the provided data reduction script for the tests of lab2_list,
modified to fix the path to gnuplot.

README contains descriptions of each of the included files, information on
outside sources used for additional information, and answers to each of the
questions.


I used stackoverflow to figure out how to make loops in Makefiles.
https://stackoverflow.com/questions/1490949/how-to-write-loop-in-a-makefile


Question 2.1.1 - causing conflicts:
	 Why does it take many iterations before errors are seen?
	 The time between loading the variable's value and storing the new value
	 back into the variable is very short. This means that the probability
	 of another thread reading the outdated value before the new value gets
	 stored back into memory is very low.
	 Why does a significantly smaller number of iterations so seldom fail?
	 With very low numbers of iterations, one thread will be significantly
	 progressed through its iterations by the time the last thread begins,
	 meaning that not only are there not many chances for each thread to
	 hit the small window of inaccuracy in the other threads because there
	 are few iterations, but because many of the iterations are run against
	 fewer than the total threads.

Question 2.1.2 - cost of yielding:
	 Why are the --yield runs so much slower?
	 When each thread yields, the processor must switch to another. This
	 means that the --yield runs switch threads much more often than the
	 normal runs, which incurs more overhead.
	 Where is the additional time going?
	 While it is less expensive to switch threads than processes, there is
	 still a significant overhead in storing the registers to memory in
	 order to switch.
	 Is it possible to get valid per-operation timings if we are using the
	 --yield option?
	 If so, explain how. If not, explain why not.
	 It is still possible to time individual operations by using
	 CLOCK_THREAD_CPUTIME_ID to ignore the time spent yielding to another
	 thread. It is also still possible to get an average per-operation
	 timing by waiting for all threads to complete.

Question 2.1.3 - measurement errors:
	 Why does the average cost per operation drop with increasing
	 iterations?
	 There is overhead incurred by getting the time as well as setting up
	 the threads. Adding more iterations causes this constant overhead to
	 have less effect on the average.
	 If the cost per iteration is a function of the number of iterations,
	 how do we know how many iterations to run (or what the "correct" cost
	 is)?
	 When the average cost per operation begins to tail off and stop
	 dropping so quickly, the effect of the overhead has become very small.
	 Thus, at higher numbers of iterations, the average cost becomes more
	 accurate.

Question 2.1.4 - costs of serialization:
	 Why do all of the options perform similarly for low numbers of threads?
	 At low numbers of threads, it is unlikely that multiple threads will be
	 attempting to use the locked part at the same time. There is a high
	 probability that each thread will simply pass through the lock with no
	 conflicts, which means that the lock only adds a small amount to the
	 time per operation.
	 Why do the three protected operations slow down as the number of
	 threads rises?
	 At higher numbers of threads, it becomings increasingly likely that
	 there are other threads attempting to use the locked portion.
	 Therefore, each thread becomes more likely to need to wait at the lock
	 until another thread completes, and the wait times become longer as
	 more threads wait at the same lock.

Question 2.2.1 - scalability of Mutex
	 Compare the variation in time per mutex-protected operation vs the
	 number of threads in Part-1 (adds) and Part-2 (sorted lists).
	 