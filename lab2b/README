NAME: Jeffrey Xu
EMAIL: jeffreyhxu@gmail.com
ID: 404768745

The tarball contains SortedList.h, SortedList.c, lab2_list.c, Makefile,
lab2b_list.csv, profile.out, lab2b_1.png, lab2b_2.png, lab2b_3.png, lab2b_4.png,
lab2b_5.png, lab2b.gp, and README.

SortedList.h is a header file provided by the assignment that contains
interfaces for linked list operations.

SortedList.c is the source for a C module that implements the insert, delete,
lookup, and length methods described in the header. This file was copied from
lab 2a.

lab2_list.c is the source for a C program that behaves as described in the spec,
using multiple threads to perform operations on a sorted linked list, with
options available to change the number of threads and iterations, yield at
specified points, use spinlocks or mutexes for synchronization, and partition
the list. It reports the total time taken, average time per operation, and
average wait time per lock.

Makefile contains the default, tests, profile, graphs, dist, and clean targets.
The default target builds lab2_list. tests runs the specified test cases and
stores the output in lab2b_list.csv. profile runs a test using pprof to profile
the performance of a spinlocked run, outputting profile.out. Note that this
target relies on my personal installation of pprof and is not usable by other
accounts. graphs uses lab2b.gp on lab2b_list.csv to generate graphs lab2b_1.png
through lab2b_5.png. dist creates the tarball. clean removes all files created
by the Makefile, namely the object files and executable, lab2b_list.csv,
profile.out, all five graphs, and the tarball.

lab2b_list.csv contains the results of all test runs except the one used to
create an execution profile for pprof.

profile.out is an execution profiling report showing where time was spent by
an un-partitioned, spinlocked run. It shows the report for the whole program as
well as which lines in the spinlock function used the most time.

lab2b_1.png shows throughput vs number of threads for mutex and spinlock
synchronized list operations. Note that in this and graphs 4 and 5, throughput
is not adjusted for increasing list length.

lab2b_2.png shows mean time per mutex wait and mean time per operation for mutex
synchronized list operations.

lab2b_3.png shows successful iterations vs threads for each synchronization
method.

lab2b_4.png shows throughput vs number of threads for mutex synchronized
partitioned lists.

lab2b_5.png shows throughput vs number of threads for spinlock synchronized
partitioned lists.

lab2b.gp uses gnuplot to generate the above five graphs from lab2_list.csv.

README contains descriptions of each included files and brief answers to each of
the questions in the assignment.

QUESTION 2.3.1 - Cycles in the basic list implementation:
	 Where do you believe most of the cycles are spent in the 1 and 2-thread
	 list tests?
	 I believe most of the cycles are spent actually running the operations
	 on the list, specifically loading the pointers to the next elements in
	 the traversals required by insertion, lookup, and length.
	 Why do you believe these to be the most expensive parts of the code?
	 The locks should not take much time because they don't need to wait for
	 many other threads, either none or 1. Even with zero lock overhead, the
	 throughput is expected to drop linearly as each operation takes longer
	 on longer lists. Traversing the list requires the most load operations
	 of any of the used list operations, so it is expected to take the most
	 time without much lock overhead.
	 Where do you believe most of the time/cycles are being spent in the
	 high-thread spin-lock tests?
	 I believe most of the cycles are being spent spinning while waiting for
	 the thread in the critical section.
	 Where do you believe most of the time/cycles are being spent in the
	 high-thread mutext tests?
	 I still believe most of the time is being spent on the list operations,
	 as the decrease in throughput is close to that expected from having
	 longer lists, but with more threads waiting, locking and unlocking may
	 take more time as the waiting lists get longer.

QUESTION 2.3.2 - Execution Profiling:
	 Where (what lines of code) are consuming most of the cycles when the
	 spin-lock version of the list exerciser is run with a large number of
	 threads?
	 The spin-lock function is consuming the most cycles, specifically the
	 atomic test-and-set operation that repeatedly sets the lock to 1.
	 Why does this operation become so expensive with large numbers of
	 threads?
	 This operation consumes the most time for two reasons. First, with more
	 threads competing to use the list, each thread has to wait longer on
	 average in the spinlock before it gets to progress. More threads
	 waiting means a lower probability for each of them to progress, since
	 only one can progress into the critical section at a time. Secondly,
	 while that explains why the spin-lock loop takes the large majority of
	 the time, it does not explain why the test-and-set takes so much more
	 time than the conditional. This is because the test-and-set, being
	 before-and-after atomic, must be completely serialized across all
	 processors. While checking the value of locked can be done on every
	 processor simultaneously, only one processor can execute a test-and-set
	 on locked at a time. The effect of this serialization grows with the
	 number of threads contending for the use of the shared variable.

QUESTION 2.3.3 - Mutex Wait Time:
	 Look at the average time per operation (vs # threads) and the average
	 wait-for-mutex time (vs # threads).
	 Why does the average lock-wait time rise so dramatically with the
	 number of contending threads?
	 As the number of contending threads goes up, each thread either has a
	 higher probability of having to wait for another thread to exit the
	 critical section or has to wait in a longer queue of threads all
	 waiting for the same lock.
	 Why does the completion time per operation rise (less dramatically)
	 with the number of contending threads?
	 The time taken per operation increases both because the list is longer
	 with more threads and thus a longer list, and because more threads
	 means a higher probability of waiting on lock acquisition instead of
	 passing through, which incurs less cost.
	 How is it possible for the wait time per operation to go up faster (or
	 higher) than the completion time per operation?
	 The wait time per operation is summed across all threads, while the
	 completion time per operation benefits from having multiple processors
	 running different threads at once. If a completely serial process is
	 split up across multiple threads with no overhead incurred by locks,
	 the wait time will go up while the completion time will stay constant.
	 In addition, wait time is influenced both by how long it takes for each
	 thread to complete an operation, relinquishing its lock, and by how
	 many threads' completion must be waited for by any one thread, whereas
	 completion time is only influenced by the former.

QUESTION 2.3.4 - Performance of Partitioned Lists
	 Explain the change in performance of the synchronized methods as a
	 function of the number of lists.
	 Performance increases as the number of lists increases because there is
	 less contention. Every time there would be contention between a pair of
	 threads in a single list, there is instead only a 1/N chance where N is
	 the number of lists that the two threads will still contend with each
	 other. Less contention means each thread has to wait for less time
	 before it can get a lock and reduces the likelihood that a thread will
	 have to undergo a costly context switch to wait for whoever currently
	 holds that lock.
	 Should the throughput continue increasing as the number of lists is
	 further increased? If not, explain why not.
	 The throughput should continue increasing with the number of lists, but
	 the gains will eventually diminish as contention nears zero. However,
	 with 16 threads, 80 iterations, and 4 lists, there is still a
	 reasonably high chance of contention between threads, so adding more
	 lists will still increase throughput.
	 It seems reasonable to suggest the throughput of an N-way partitioned
	 list should be equivalent to the throughput of a single list with
	 fewer (1/N) threads. Does this appear to be true in the above curves?
	 If not, explain why not.
	 This does not appear to be true. When not adjusted for increased list
	 length (as in the submitted graphs), the 8-list, 12-thread test still
	 had better throughput than the single-list, single-thread test, and the
	 16-list, 8-thread test had better throughput than either the 8-list,
	 4-thread test or the 4-list, 2-thread test. This is because there are
	 multiple ways that partitioning the list aids performance. Assuming
	 partitioning the list helps performance exactly as much as reducing the
	 number of threads comes from the fact that partitioning the list
	 reduces contention about as much as reducing the number of threads
	 does. However, reducing contention while keeping the number of threads
	 high means we are able to use genuine parallelism across separate
	 lists. Suppose we had some very high number of lists, enough to
	 completely eliminate contention between threads. Then we would expect
	 N threads with this very high number of lists to have a throughput N
	 times higher than a single thread with a single list. In addition,
	 partitioning the list also makes each list shorter, which speeds up
	 the list traversals required by insertion, lookup, and length
	 operations.